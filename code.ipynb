{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EwIqSMHb27Q"
      },
      "outputs": [],
      "source": [
        "  ################ code for supervised model with cross validation in Python\n",
        "\n",
        "from google.colab import files\n",
        "df_new= files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('df_interpolated.csv')\n",
        "\n",
        "splitting the response and features\n",
        "X = df.drop('response',axis='columns')\n",
        "y = df['response']\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'df' is your original DataFrame\n",
        "X1 = X.copy()\n",
        "\n",
        "# Define your categorical columns\n",
        "categorical_columns = [ 'ajcc_pathologic_tumor_stage', 'ajcc_pathologic_t','ajcc_pathologic_n','ajcc_pathologic_m',\n",
        "                       'brca_subtype', 'surg\n",
        "                       ical_procedure_first', 'margin_status',\n",
        "                       'er_status_by_ihc', 'pr_s\n",
        "                       tatus_by_ihc', 'her2_status_by_ihc']\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to each categorical column\n",
        "for column in categorical_columns:\n",
        "    # The encoder fits on the data and\n",
        "    then transforms it to labels\n",
        "    X1[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X1 is your original DataFrame\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the data\n",
        "scaled_array = min_max_scaler.fit_transform(X1)\n",
        "\n",
        "# Convert the scaled array back into a DataFrame\n",
        "X1_scaled = pd.DataFrame(scaled_array, columns=\n",
        "X1.columns, index=X1.index)\n",
        "\n",
        "# If you want to overwrite the original X1 DataFram\n",
        "e with the scaled data\n",
        "X1 = X1_scaled\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#balancing the classes\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_repo\n",
        "rt, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Apply SMOTE to oversample the minority class\n",
        "in the entire dataset\n",
        "smote = SMOTE(sampling_strategy=0.3)\n",
        "X1_sm, y_sm = smote.fit_resample(X1, y)\n",
        "\n",
        "# Now split the SMOTE-enhanced data into training a\n",
        "nd testing sets\n",
        "X1_train_sm, X1_test_sm, y_train_sm,\n",
        "y_test_sm = train_test_split(X1_sm, y_sm, test_size\n",
        "=0.2, random_state=42)\n",
        "\n",
        "#perfroming 5-fold cv\n",
        "from sklearn.metrics import roc_curve, auc, class\n",
        "ification_report\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize variables for ROC plot\n",
        "fprs, tprs, aucs, labels = [], [], [], []\n",
        "\n",
        "# Iterate over classifiers\n",
        "for name, classifier in classifiers:\n",
        "    print(f\"Results for {name}:\")\n",
        "\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    predicted_probs = []\n",
        "\n",
        "    # Perform cross-validation\n",
        "    for train_index, test_index in cv.split(X1, y1):\n",
        "        X_train, X_test = X1.iloc[train_inde\n",
        "        x], X1.iloc[test_index]\n",
        "        y_train, y_test = y1.iloc[train_inde\n",
        "        x], y1.iloc[test_index]\n",
        "\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "        y_prob = classifier.predict_proba(X_test)\n",
        "        [:, 1]\n",
        "        if hasattr(classifier, \"predict_proba\")\n",
        "        else [0] * len(y_pred)\n",
        "\n",
        "        true_labels.extend(y_test)\n",
        "        predicted_labels.extend(y_pred)\n",
        "        predicted_probs.extend(y_prob)\n",
        "\n",
        "    # Classification report\n",
        "    print(classification_report(true_labels,\n",
        "    predicted_labels))\n",
        "\n",
        "    # ROC Curve and AUC\n",
        "    if hasattr(classifier, \"predict_proba\"):\n",
        "        fpr, tpr, _ = roc_curve(true_labels,\n",
        "        predicted_probs)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        fprs.append(fpr)\n",
        "        tprs.append(tpr)\n",
        "        aucs.append(roc_auc)\n",
        "        labels.append(f\"{name} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "# Plotting all ROC curves in one figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "for fpr, tpr, label in zip(fprs, tprs, labels):\n",
        "    plt.plot(fpr, tpr, lw=2, label=label)\n",
        "plt.plot([0, 1], [0, 1], color='gray',\n",
        "linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic\n",
        "(ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "##########unsupervised model\n",
        "from google.colab import files\n",
        "data = files.upload()\n",
        "\n",
        "import pandas as pd\n",
        "df = pd.read_csv('gene_names22k.csv')\n",
        "import numpy as np\n",
        "import scanpy as sc\n",
        "\n",
        "df[~np.isfinite(df)] = np.nan\n",
        "\n",
        "# Create an AnnData object\n",
        "adata = sc.AnnData(df)\n",
        "\n",
        "# Perform preprocessing steps\n",
        "(e.g., normalization, scaling)\n",
        "sc.pp.normalize_total(adata)  # Normalization\n",
        "sc.pp.log1p(adata)  # Logarithmize the data\n",
        "\n",
        "# Perform dispersion analysis\n",
        "sc.pp.highly_variable_genes(adata, n_t\n",
        "op_genes=2000)\n",
        "\n",
        "# Get the top 2000 variable genes\n",
        "top_variable_genes =\n",
        "adata.var.highly_variable.index.tolist()\n",
        "\n",
        "highly_variable_mask =\n",
        "adata.var['highly_variable']\n",
        "\n",
        "# Filter the data matrix to\n",
        "keep only the highly variable genes\n",
        "highly_variable_genes_data =\n",
        "adata[:, highly_variable_mask]\n",
        "\n",
        "# Access the data matrix of\n",
        "the highly variable genes\n",
        "highly_variable_genes_matrix = highly_variable_genes_data.X\n",
        "\n",
        "# Get the gene names corresponding\n",
        "to highly variable genes\n",
        "gene_names = adata.var_names[highly_variable_mask]\n",
        "\n",
        "# Create a DataFrame with gene names\n",
        "and expression values\n",
        "expression_df = pd.DataFrame\n",
        "(data=highly_variable_genes_matrix, columns=gene_names)\n",
        "\n",
        "# Display the DataFrame head to\n",
        "view gene names and expression values\n",
        "print(expression_df.head())\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'expression_df' is the\n",
        "DataFrame containing gene expression data\n",
        "\n",
        "# Perform PCA on the highly variable genes\n",
        "pca = PCA()\n",
        "pca_result = pca.fit_transform(expression_df)\n",
        "\n",
        "# Access and display the principal components\n",
        "principal_components = pca.components_\n",
        "\n",
        "# Convert principal components to a\n",
        "DataFrame for better visualization\n",
        "pc_df = pd.DataFrame(principal_components,\n",
        "columns=expression_df.columns)\n",
        "\n",
        "# Display the first few principal components\n",
        "print(\"Principal Components:\")\n",
        "print(pc_df.head())\n",
        "\n",
        "\n",
        "pca = PCA()\n",
        "pca_result = pca.fit(expression_df)\n",
        "\n",
        "# Calculate cumulative explained variance ratio\n",
        "cumulative_variance =\n",
        "np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Set the desired threshold (e.g., 95% of variance)\n",
        "threshold = 0.95\n",
        "\n",
        "# Find the number of components to retain\n",
        "num_components =\n",
        "np.argmax(cumulative_variance >= threshold) + 1\n",
        "\n",
        "print(f\"Number of components to retain\n",
        "{threshold * 100}% variance: {num_components}\")\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming 'expression_df' is the\n",
        "DataFrame containing gene expression data\n",
        "\n",
        "# Number of components to retain\n",
        "num_components = 2\n",
        "\n",
        "# Perform PCA on the highly variabl\n",
        "e genes with wcomponents\n",
        "pca = PCA(n_components=num_components)\n",
        "pca_result = pca.fit_transform(expression_df)\n",
        "\n",
        "# Access and display the shape of the\n",
        "reduced PCA-transformed data\n",
        "print(f\"Reduced dimension PCA-transformed data shape: {pca_result.shape}\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming 'pca_result' is the\n",
        "PCA-transformed data with shape (1200, 445)\n",
        "\n",
        "# Convert PCA-transformed data to\n",
        "a DataFrame for easier viewing\n",
        "pca_df = pd.DataFrame(pca_result)\n",
        "\n",
        "# Display the first few rows of the PCA-transformed data\n",
        "print(pca_df.head())\n",
        "\n",
        "\n",
        "#printing the k means plots\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import\n",
        "silhouette_score, silhouette_samples\n",
        "import numpy as np\n",
        "\n",
        "# Assuming pca_df contains the\n",
        "PCA-transformed data with cluster labels\n",
        "\n",
        "# Extract PCA components used for clustering\n",
        "cluster_data = pca_df[['PC1', 'PC2']]\n",
        "\n",
        "# Choose the number of clusters\n",
        "num_clusters = 5  # You can adjust this number\n",
        "\n",
        "# K-means clustering\n",
        "kmeans =\n",
        "KMeans(n_clusters=num_clusters, random_state=42)\n",
        "cluster_labels = kmeans.fit_predict(cluster_data)\n",
        "pca_df['Cluster'] = cluster_labels\n",
        "\n",
        "# Get the silhouette score\n",
        "silhouette_avg =\n",
        "silhouette_score(cluster_data, cluster_labels)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "# Compute the silhouette scores for each sample\n",
        "sample_silhouette_values =\n",
        "silhouette_samples(cluster_data, cluster_labels)\n",
        "\n",
        "# Create a subplot with the silhouette plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(1, 1, 1)\n",
        "\n",
        "y_lower = 10\n",
        "for i in range(num_clusters):\n",
        "    # Aggregate the silhouette scores\n",
        "    for samples belonging to cluster i and sort them\n",
        "    ith_cluster_silhouette_values\n",
        "    = sample_silhouette_values[cluster_labels == i]\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "\n",
        "    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_cluster_i\n",
        "\n",
        "    # Fill the silhouette plot\n",
        "    color = plt.cm.get_cmap(\"Spectral\")(i / num_clusters)\n",
        "    plt.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                      0, ith_cluster_silhouette_values,\n",
        "                      facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "    # Label the silhouette plots with their cluster numbers\n",
        "    plt.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "    # Compute the new y_lower for the next plot\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "plt.title(\"Silhouette Plot\")\n",
        "plt.xlabel(\"Silhouette coefficient values\")\n",
        "plt.ylabel(\"Cluster label\")\n",
        "plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "plt.yticks([])\n",
        "plt.show()\n",
        "\n",
        "# Visualize centroids of the clusters in the PCA plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(num_clusters):\n",
        "    cluster_center = kmeans.cluster_centers_[i]\n",
        "    plt.scatter(cluster_center[0],\n",
        "    cluster_center[1], marker='o', s=200, label=f'Centroid {i}')\n",
        "\n",
        "plt.scatter(cluster_data['PC1'],\n",
        "cluster_data['PC2'],\n",
        "c=cluster_labels, cmap='viridis', alpha=0.5)\n",
        "plt.title('PCA Plot with Cluster Centroids')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "###improved models\n",
        "X = df.drop('response',axis='columns')\n",
        "y = df['response']\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming 'df' is your original DataFrame\n",
        "X1 = X.copy()\n",
        "\n",
        "# Define your categorical columns\n",
        "categorical_columns =\n",
        "['ajcc_pathologic_t', 'ajcc_pathologic_n', 'ajcc_pathologic_m',\n",
        "\n",
        "                       'brca_subtype', 'surgical_procedure_first',\n",
        "                       'margin_status',\n",
        "                       'er_status_by_ihc', 'pr_status_by_ihc',\n",
        "                       'her2_status_by_ihc',\n",
        "                       'ajcc_pathologic_tumor_stage']\n",
        "\n",
        "# Initialize the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to each categorical column\n",
        "for column in categorical_columns:\n",
        "    # The encoder fits on the data and\n",
        "    then transforms it to labels\n",
        "    X1[column] = label_encoder.fit_transform(X[column])\n",
        "\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming X1 is your original DataFrame\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Scale the data\n",
        "scaled_array = min_max_scaler.fit_transform(X1)\n",
        "\n",
        "# Convert the scaled array back into a DataFrame\n",
        "X1_scaled = pd.DataFrame(scaled_array,\n",
        "columns=X1.columns, index=X1.index)\n",
        "\n",
        "# If you want to overwrite the original\n",
        "X1 DataFrame with the scaled data\n",
        "X1 = X1_scaled\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf.fit(X1, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Sort the feature importances in\n",
        "descending order and select the top 100\n",
        "indices = np.argsort(importances)[::-1]\n",
        "top_indices = indices[:100]\n",
        "\n",
        "# Get the names of the top features\n",
        "top_features = X1.columns[top_indices]\n",
        "\n",
        "# Print the top features\n",
        "print(\"Top 100 features:\")\n",
        "print(top_features)\n",
        "X1_top_features = X1[top_features]\n",
        "X2 = X1_top_features\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X2_train, X2_test, y_train,\n",
        "y_test = train_test_split(X2, y, test_size=0.3,\n",
        "random_state=42)\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "smote = SMOTE(sampling_strategy=0.3)\n",
        "X2_sm, y_sm = smote.fit_resample(X2, y)\n",
        "\n",
        "# Check the value counts after oversampling\n",
        "print(pd.Series(y_sm).value_counts())\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report,\n",
        "roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "\n",
        "\n",
        "# List to store mean AUC scores for each classifier\n",
        "mean_auc_scores = []\n",
        "\n",
        "# Train and evaluate each classifier\n",
        "plt.figure(figsize=(10, 8))\n",
        "for name, classifier in classifiers:\n",
        "    classifier.fit(X2_train, y_train)\n",
        "    y_pred = classifier.predict(X2_test)\n",
        "\n",
        "    # Classification report\n",
        "    print(f\"Classification report for {name}:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Calculate ROC curve and AUC\n",
        "    y_score = classifier.predict_proba(X2_test)[:, 1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plot ROC curve\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    # Perform k-fold cross-validation and compute mean AUC\n",
        "    auc_scores = cross_val_score(classifier,\n",
        "    X2_train, y_train, cv=5, scoring='roc_auc')\n",
        "    mean_auc = np.mean(auc_scores)\n",
        "    mean_auc_scores.append((name, mean_auc))\n",
        "\n",
        "    print(f\"Mean AUC for {name}: {mean_auc:.2f}\")\n",
        "\n",
        "# Plot settings\n",
        "plt.plot([0, 1], [0, 1], color='gray',\n",
        "linestyle='--')\n",
        "# Random guess line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n",
        "\n",
        "# Plotting mean AUC scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "for name, mean_auc in mean_auc_scores:\n",
        "    plt.bar(name, mean_auc, label=f'{name}\n",
        "    (Mean AUC = {mean_auc:.2f})')\n",
        "\n",
        "plt.xlabel('Classifier')\n",
        "plt.ylabel('Mean AUC')\n",
        "plt.title('Mean AUC Scores using 5-fold Cross-Validation')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yh4KKwI9ceuP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}